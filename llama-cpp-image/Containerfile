FROM quay.io/fedora/fedora:41 as builder

COPY oneAPI.repo /etc/yum.repos.d/

RUN dnf install -y procps-ng g++ cmake git libcurl-devel intel-oneapi-mkl-sycl-devel intel-oneapi-dnnl-devel intel-oneapi-compiler-dpcpp-cpp intel-level-zero oneapi-level-zero oneapi-level-zero-devel intel-compute-runtime ; \
    source /opt/intel/oneapi/setvars.sh ; \
    git clone https://github.com/ggerganov/llama.cpp.git ; \
    cd llama.cpp ; \
    mkdir -p build ; \
    cd build ; \
    cmake .. -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DLLAMA_CURL=ON -DGGML_CCACHE=OFF -DGGML_NATIVE=OFF ; \
    cmake --build . --config Release -j -v ; \
    cmake --install . --prefix /llama-cpp ; \
    cd ../.. ; \
    git clone https://github.com/ggerganov/whisper.cpp ; \
    cd whisper.cpp ; \
    mkdir -p build ; \
    cd build ; \
    cmake .. -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DGGML_CCACHE=OFF -DGGML_NATIVE=OFF ; \
    cmake --build . --config Release -v ; \
    cmake --install . --prefix /llama-cpp

FROM quay.io/fedora/fedora:41

COPY --from=builder /llama-cpp/bin/ /usr/bin/
COPY --from=builder /llama-cpp/lib/ /usr/lib/
COPY --from=builder /llama-cpp/lib64/ /usr/lib64/
COPY --from=builder /llama-cpp/include/ /usr/include/
COPY oneAPI.repo /etc/yum.repos.d/
COPY --chown=0:0 entrypoint.sh /

RUN dnf install -y procps-ng python3 python3-pip python3-devel intel-level-zero oneapi-level-zero intel-compute-runtime libcurl lspci clinfo intel-oneapi-runtime-compilers intel-oneapi-mkl-core intel-oneapi-mkl-sycl-blas intel-oneapi-runtime-dnnl ; \
    chown 0:0 /etc/passwd ; \
    chown 0:0 /etc/group ; \
    chmod g=u /etc/passwd /etc/group /home ; \
    chmod +x /entrypoint.sh

USER 10000

ENTRYPOINT ["/entrypoint.sh"]
CMD [ "tail", "-f", "/dev/null" ]