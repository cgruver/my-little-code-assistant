FROM quay.io/fedora/fedora:42 as builder

ARG LLAMA_CPP_REPO
ARG LLAMA_CPP_VER


COPY oneAPI.repo /etc/yum.repos.d/

RUN dnf install -y awk procps-ng g++ cmake git libcurl-devel intel-oneapi-mkl-sycl-devel intel-oneapi-dnnl-devel intel-oneapi-compiler-dpcpp-cpp intel-level-zero oneapi-level-zero oneapi-level-zero-devel intel-compute-runtime ; \
    source /opt/intel/oneapi/setvars.sh ; \
    git clone ${LLAMA_CPP_REPO} -b ${LLAMA_CPP_VER} ; \
    cd llama.cpp ; \
    mkdir -p build ; \
    cd build ; \
    cmake .. -DGGML_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DLLAMA_CURL=ON -DGGML_CCACHE=OFF -DGGML_NATIVE=ON -DGGML_CPU_AARCH64=OFF ; \
    cmake --build . --config Release -j -v ; \
    cmake --install . --prefix /llama-cpp

FROM quay.io/fedora/fedora:42

COPY --from=builder /llama-cpp/bin/ /usr/bin/
COPY --from=builder /llama-cpp/lib64/ /usr/lib64/
COPY --from=builder /llama-cpp/include/ /usr/include/
COPY oneAPI.repo /etc/yum.repos.d/
COPY --chown=0:0 entrypoint.sh /

RUN dnf install -y awk procps-ng intel-level-zero oneapi-level-zero intel-compute-runtime libcurl lspci clinfo intel-oneapi-runtime-compilers intel-oneapi-mkl-core intel-oneapi-mkl-sycl-blas intel-oneapi-runtime-dnnl intel-gpu-tools wget ; \
    chown 0:0 /etc/passwd ; \
    chown 0:0 /etc/group ; \
    chmod g=u /etc/passwd /etc/group /home ; \
    chmod +x /entrypoint.sh

USER 10000

ENTRYPOINT ["/entrypoint.sh"]
